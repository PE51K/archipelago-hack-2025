{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12664748,"sourceType":"datasetVersion","datasetId":8003296},{"sourceId":12665375,"sourceType":"datasetVersion","datasetId":8003680}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def sliced_wbf_predict(im, model, conf, overlap, wbf_iou):\n    \"\"\"\n    Runs sliced prediction on an image, applies Weighted Boxes Fusion (WBF),\n    and returns normalized bounding boxes.\n\n    Args:\n        im (np.ndarray): Input image.\n        model: YOLO model instance.\n        conf (float): Confidence threshold.\n        overlap (float): Overlap ratio for slicing.\n        wbf_iou (float): IOU threshold for WBF. \n\n    Returns:\n        list[dict]: List of predicted bounding boxes with normalized coordinates.\n    \"\"\"\n    h, w, _ = im.shape\n    # Run sliced prediction using SAHI\n    r = get_sliced_prediction(\n        im, \n        model,\n        slice_height=IMGZ, \n        slice_width=IMGZ,\n        overlap_height_ratio=overlap, \n        overlap_width_ratio=overlap,\n        conf=conf, \n        device=DEVICE, \n        verbose=False\n    )\n\n    bxs, scs = [], []\n    # Collect bounding boxes and scores\n    for o in r.object_prediction_list:\n        x1, y1, x2, y2 = o.bbox.to_xyxy()\n        bxs.append([x1 / w, y1 / h, x2 / w, y2 / h])\n        scs.append(o.score.value)\n\n    if not bxs:\n        return []\n    \n    # Apply Weighted Boxes Fusion\n    bxs, scs, _ = weighted_boxes_fusion(\n        [bxs], [scs], [[0]],\n        iou_thr=wbf_iou, skip_box_thr=SKIP_BOX_T\n    )\n\n    # Format predictions as dicts\n    return [\n        dict(\n            xc=(x1 + x2) / 2,\n            yc=(y1 + y2) / 2,\n            w=x2 - x1,\n            h=y2 - y1,\n            label=0,\n            score=s\n        ) \n        for (x1, y1, x2, y2), s \n        in zip(bxs, scs)\n    ]\n\ndef run_validation(model, conf, overlap, wbf_iou):\n    \"\"\"\n    Runs inference on all validation images, collects predictions,\n    and returns a DataFrame.\n\n    Args:\n        model: YOLO model instance.\n        conf (float): Confidence threshold.\n        overlap (float): Overlap ratio for slicing.\n        wbf_iou (float): IOU threshold for WBF.\n\n    Returns:\n        pd.DataFrame: DataFrame of predictions for all images.\n    \"\"\"\n    rows = []\n    for p in VAL_IMGDIR.glob(\"*.*\"):\n        img_id = p.stem\n        # Read and convert image to RGB\n        im     = cv2.cvtColor(cv2.imread(str(p), -1), cv2.COLOR_BGR2RGB)\n        start  = time.time()\n        preds  = sliced_wbf_predict(im, model, conf, overlap, wbf_iou)\n        dt     = round(time.time() - start, 5)\n\n        if preds:\n            # Add predictions to rows\n            for d in preds:\n                d |= dict(\n                    image_id=img_id, \n                    time_spent=dt,\n                    w_img=im.shape[1], \n                    h_img=im.shape[0]\n                )\n                rows.append(d)\n        else:\n            # If no predictions, add empty row\n            rows.append(\n                dict(\n                    image_id=img_id, \n                    xc=None, \n                    yc=None, \n                    w=None, \n                    h=None,\n                    label=0, \n                    score=None, \n                    time_spent=dt,\n                    w_img=im.shape[1], \n                    h_img=im.shape[0]\n                )\n            )\n            \n    # Return predictions as DataFrame\n    return pd.DataFrame(\n        rows, \n        columns=[\n            \"image_id\",\n            \"label\",\n            \"xc\",\n            \"yc\",\n            \"w\",\n            \"h\",\n            \"w_img\",\n            \"h_img\",\n            \"score\",\n            \"time_spent\"\n        ]\n    )\n\ndef fbeta(df_pred):\n    \"\"\"\n    Computes the F-beta score for predictions using the custom metric.\n\n    Args:\n        df_pred (pd.DataFrame): DataFrame of predictions.\n\n    Returns:\n        float: F-beta score.\n    \"\"\"\n    pred_bytes = metric.df_to_bytes(df_pred)\n    gt_bytes   = metric.open_df_as_bytes(str(VAL_GTCSV))\n    score, *_  = metric.evaluate(pred_bytes, gt_bytes, beta=BETA, parallelize=False)\n    return score\n\ndef objective(trial):\n    \"\"\"\n    Optuna objective function for hyperparameter search.\n\n    Args:\n        trial (optuna.trial.Trial): Optuna trial object.\n\n    Returns:\n        float: F-beta score for current hyperparameters.\n    \"\"\"\n    conf     = trial.suggest_float(\"conf\",     0.05, 0.40)\n    overlap  = trial.suggest_float(\"overlap\",  0.10, 0.35)\n    wbf_iou  = trial.suggest_float(\"wbf_iou\",  0.45, 0.70)\n    df_pred  = run_validation(model, conf, overlap, wbf_iou)\n    return fbeta(df_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ultralytics clearml pyyaml sahi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:28:49.445403Z","iopub.execute_input":"2025-08-08T21:28:49.445649Z","iopub.status.idle":"2025-08-08T21:30:16.128278Z","shell.execute_reply.started":"2025-08-08T21:28:49.445624Z","shell.execute_reply":"2025-08-08T21:30:16.127537Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.176-py3-none-any.whl.metadata (37 kB)\nCollecting clearml\n  Downloading clearml-2.0.2-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nCollecting sahi\n  Downloading sahi-0.11.32-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.15-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (25.3.0)\nCollecting furl>=2.0.0 (from clearml)\n  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (4.24.0)\nCollecting pathlib2>=2.3.0 (from clearml)\n  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from clearml) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.9.0.post0)\nRequirement already satisfied: pyjwt<2.11.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.10.1)\nRequirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.17.0)\nRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.5.0)\nRequirement already satisfied: referencing<0.40 in /usr/local/lib/python3.11/dist-packages (from clearml) (0.36.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sahi) (8.2.1)\nCollecting fire (from sahi)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting pybboxes==0.1.6 (from sahi)\n  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (2.1.1)\nCollecting terminaltables (from sahi)\n  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting orderedmultidict>=1.0.1 (from furl>=2.0.0->clearml)\n  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (2025.4.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (0.25.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing<0.40->clearml) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->sahi) (3.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.176-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading clearml-2.0.2-py2.py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sahi-0.11.32-py3-none-any.whl (113 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.5/113.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\nDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\nDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.15-py3-none-any.whl (28 kB)\nDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\nDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=73cf838abfc0902e6b891fbbaed03dff440747be059768a256c1ce496cda82a4\n  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\nSuccessfully built fire\nInstalling collected packages: terminaltables, pathlib2, orderedmultidict, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, furl, nvidia-cusolver-cu12, ultralytics-thop, pybboxes, ultralytics, sahi, clearml\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clearml-2.0.2 fire-0.7.0 furl-2.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 orderedmultidict-1.0.1 pathlib2-2.3.7.post1 pybboxes-0.1.6 sahi-0.11.32 terminaltables-3.1.10 ultralytics-8.3.176 ultralytics-thop-2.0.15\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple\nfrom numba import jit\nfrom concurrent.futures import ThreadPoolExecutor\nimport torch\nimport cv2\nimport albumentations as A\nfrom pathlib import Path\nfrom ultralytics import YOLO\nimport shutil\nfrom tqdm import tqdm\nimport yaml\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:30:43.450948Z","iopub.execute_input":"2025-08-08T21:30:43.451768Z","iopub.status.idle":"2025-08-08T21:30:51.733702Z","shell.execute_reply.started":"2025-08-08T21:30:43.451734Z","shell.execute_reply":"2025-08-08T21:30:51.733185Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"âœ… Custom metric code is embedded in the main script.\")\n\nCOLUMNS = ['image_id', 'label', 'xc', 'yc', 'w', 'h', 'w_img', 'h_img', 'score', 'time_spent']\n\ndef df_to_bytes(df: pd.DataFrame) -> bytes: return df.to_json().encode(encoding=\"utf-8\")\ndef bytes_to_df(df_byte: bytes) -> pd.DataFrame: return pd.DataFrame(json.loads(df_byte.decode(\"utf-8\").replace(\"'\", '\"')))\ndef open_df_as_bytes(csv_path: str) -> bytes: return df_to_bytes(pd.read_csv(csv_path, sep=\",\", decimal=\".\", converters={'image_id': str, 'time_spent': float}))\ndef set_types(df: pd.DataFrame) -> pd.DataFrame: return df.astype({'image_id': str, 'label': int, 'xc': float, 'yc': float, 'w': float, 'h': float, 'w_img': int, 'h_img': int}, errors='ignore')\n\ndef get_time_spent(df: pd.DataFrame, all_image_ids: list) -> np.ndarray:\n    time_spent_map = df.groupby('image_id')['time_spent'].first()\n    return np.array([time_spent_map.get(img_id, 0) for img_id in all_image_ids])\n\ndef preprocess_predicted_df(predicted_file: bytes, gt_file: bytes, all_image_ids: list):\n    predicted_df = bytes_to_df(predicted_file)\n    gt_df = bytes_to_df(gt_file)\n    time_spent = get_time_spent(predicted_df, all_image_ids)\n    \n    gt_df = set_types(gt_df).set_index('image_id').sort_index()\n\n    if not predicted_df.empty and 'score' in predicted_df.columns and predicted_df['score'].iloc[0] != -1:\n        predicted_df = predicted_df.drop(columns=['time_spent'])\n        predicted_df = set_types(predicted_df).set_index('image_id').sort_index()\n    else:\n        predicted_df = pd.DataFrame(columns=gt_df.columns).set_index(pd.Index([], name='image_id'))\n        \n    return gt_df, predicted_df, time_spent\n\ndef get_box_coordinates(row):\n    w_img, h_img = int(row['w_img']), int(row['h_img'])\n    x1, y1 = int((row['xc'] - row['w']/2) * w_img), int((row['yc'] - row['h']/2) * h_img)\n    x2, y2 = int((row['xc'] + row['w']/2) * w_img), int((row['yc'] + row['h']/2) * h_img)\n    return (x1, y1, x2, y2)\n\n@jit(nopython=True)\ndef compute_iou_from_coords(pred_box, gt_box):\n    x1_p, y1_p, x2_p, y2_p = pred_box\n    x1_g, y1_g, x2_g, y2_g = gt_box\n    x_left, y_top = max(x1_p, x1_g), max(y1_p, y1_g)\n    x_right, y_bottom = min(x2_p, x2_g), min(y2_p, y2_g)\n    if x_right < x_left or y_bottom < y_top: return 0.0\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    box1_area = (x2_p - x1_p) * (y2_p - y1_p)\n    box2_area = (x2_g - x1_g) * (y2_g - y2_g)\n    union_area = box1_area + box2_area - intersection_area\n    return intersection_area / union_area if union_area > 0 else 0.0\n\ndef process_image(pred_df, gt_df, thresholds):\n    pred_boxes = [get_box_coordinates(row) for _, row in pred_df.iterrows()]\n    gt_boxes = [get_box_coordinates(row) for _, row in gt_df.iterrows()]\n    iou_matrix = np.zeros((len(pred_boxes), len(gt_boxes)))\n    for i, p_box in enumerate(pred_boxes):\n        for j, g_box in enumerate(gt_boxes): iou_matrix[i, j] = compute_iou_from_coords(p_box, g_box)\n    \n    results = {}\n    for t in thresholds:\n        matches, iou_mat = [], iou_matrix.copy()\n        iou_mat[iou_mat < t] = 0\n        pred_indices, gt_indices = set(), set()\n        while iou_mat.max() > 0:\n            i, j = np.unravel_index(np.argmax(iou_mat), iou_mat.shape)\n            if i not in pred_indices and j not in gt_indices:\n                pred_indices.add(i); gt_indices.add(j); matches.append((i, j))\n            iou_mat[i, :], iou_mat[:, j] = 0, 0\n        tp = len(matches)\n        results[t] = {'tp': tp, 'fp': len(pred_boxes) - tp, 'fn': len(gt_boxes) - tp}\n    return results\n\ndef metric_counter(time_spent, total_tp, total_fp, total_fn, thresholds, beta, m):\n    f_beta_scores, beta_squared = [], beta ** 2\n    for t in thresholds:\n        tp, fp, fn = total_tp[t], total_fp[t], total_fn[t]\n        numerator = (1 + beta_squared) * tp\n        denominator = (1 + beta_squared) * tp + beta_squared * fn + fp\n        f_beta_scores.append(numerator / denominator if denominator > 0 else 0.0)\n    return np.mean(f_beta_scores), round(float(np.mean(time_spent)), 3)\n\ndef compute_overall_metric(predicted_df, gt_df, time_spent, thresholds, beta, m, parallelize=True):\n    unique_image_ids = gt_df.index.unique()\n    total_tp, total_fp, total_fn = {t: 0 for t in thresholds}, {t: 0 for t in thresholds}, {t: 0 for t in thresholds}\n\n    def process_image_id(image_id):\n        pred_df_img = predicted_df.loc[[image_id]] if image_id in predicted_df.index else pd.DataFrame()\n        gt_df_img = gt_df.loc[[image_id]]\n        if pred_df_img.empty: return {t: {'tp': 0, 'fp': 0, 'fn': len(gt_df_img)} for t in thresholds}\n        return process_image(pred_df_img, gt_df_img, thresholds)\n\n    results = list(ThreadPoolExecutor().map(process_image_id, unique_image_ids))\n    for result in filter(None, results):\n        for t in thresholds:\n            total_tp[t] += result[t]['tp']; total_fp[t] += result[t]['fp']; total_fn[t] += result[t]['fn']\n    return metric_counter(time_spent, total_tp, total_fp, total_fn, thresholds, beta, m)\n\ndef evaluate(predicted_file, gt_file, all_image_ids, thresholds=np.round(np.arange(0.3, 1.0, 0.07), 2), beta=1.0):\n    try:\n        gt_df, predicted_df, time_spent = preprocess_predicted_df(predicted_file, gt_file, all_image_ids)\n        m = len(all_image_ids)\n        return compute_overall_metric(predicted_df, gt_df, time_spent, thresholds, beta, m)\n    except Exception as e:\n        print(f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² evaluate: {e}\"); return 0.0, 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:33:29.813619Z","iopub.execute_input":"2025-08-08T21:33:29.814102Z","iopub.status.idle":"2025-08-08T21:33:29.944009Z","shell.execute_reply.started":"2025-08-08T21:33:29.814077Z","shell.execute_reply":"2025-08-08T21:33:29.943299Z"}},"outputs":[{"name":"stdout","text":"âœ… Custom metric code is embedded in the main script.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def run_inference_for_metric(model, image_paths):\n    print(\"INFO: Starting inference run for custom metric...\")\n    results_list = []\n    \n    for img_path in tqdm(image_paths, desc=\"Custom Validation\"):\n        start_time = time.time()\n        preds = model.predict(source=str(img_path), verbose=False)\n        time_spent = time.time() - start_time\n        \n        h_img, w_img = preds[0].orig_shape\n        \n        if len(preds[0].boxes) == 0:\n            results_list.append({'image_id': img_path.name, 'label': -1, 'xc': -1, 'yc': -1, 'w': -1, 'h': -1, 'w_img': w_img, 'h_img': h_img, 'score': -1, 'time_spent': time_spent})\n        else:\n            for box in preds[0].boxes:\n                x_c, y_c, w, h = box.xywhn[0]\n                results_list.append({'image_id': img_path.name, 'label': int(box.cls), 'xc': float(x_c), 'yc': float(y_c), 'w': float(w), 'h': float(h), 'w_img': w_img, 'h_img': h_img, 'score': float(box.conf), 'time_spent': time_spent})\n    \n    return pd.DataFrame(results_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:33:33.344717Z","iopub.execute_input":"2025-08-08T21:33:33.345023Z","iopub.status.idle":"2025-08-08T21:33:33.351099Z","shell.execute_reply.started":"2025-08-08T21:33:33.344999Z","shell.execute_reply":"2025-08-08T21:33:33.350395Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"best_custom_score = -1.0\nground_truth_bytes = None\n\ndef custom_metric_callback(trainer):\n    global best_custom_score, ground_truth_bytes, VAL_IMAGE_DIR, API_configured, MODEL_DATASET_SLUG\n    if ground_truth_bytes is None: return\n    \n    print(\"\\n\" + \"=\"*20 + \" Custom Metric Validation \" + \"=\"*20)\n    \n    val_image_paths = sorted(list(Path(VAL_IMAGE_DIR).glob(\"*.jpg\")))\n    all_val_image_names = [p.name for p in val_image_paths]\n    \n    predictions_df = run_inference_for_metric(trainer.model, val_image_paths)\n    predictions_bytes = df_to_bytes(predictions_df)\n    \n    metric, avg_time = evaluate(predicted_file=predictions_bytes, gt_file=ground_truth_bytes, all_image_ids=all_val_image_names)\n    print(f\"ðŸ“ˆ Custom Metric Score (F-beta): {metric:.5f}\")\n    \n    trainer.logger.log_metrics({\"custom_F_beta\": metric}, step=trainer.epoch)\n    if metric > best_custom_score:\n        best_custom_score = metric\n        print(f\"ðŸš€ New best model found! F-beta={best_custom_score:.5f}. Saving and uploading...\")\n\n        local_save_dir = Path(trainer.save_dir)\n        local_save_path = local_save_dir / \"best_by_metric.pt\"\n        trainer.model.save(local_save_path)\n\n        if API_configured:\n            try:\n                version_notes = f\"Block {current_block_idx}, Epoch {trainer.epoch}, F-beta: {metric:.5f}\"\n                upload_dir = Path(\"/kaggle/working/upload_temp\")\n                if upload_dir.exists(): shutil.rmtree(upload_dir)\n                upload_dir.mkdir()\n                shutil.copy(local_save_path, upload_dir / f\"best_model_b{current_block_idx}_e{trainer.epoch}.pt\")\n\n                subprocess.run(f\"kaggle datasets version -p {upload_dir} -m '{version_notes}' -r zip\", shell=True, check=True, capture_output=True)\n                print(\"âœ… Successfully uploaded new best model to Kaggle Datasets.\")\n            except Exception as e:\n                print(f\"âŒ ERROR uploading model to Kaggle: {e}\")\n        else:\n            print(\"SKIPPED UPLOAD: Kaggle API not configured.\")\n    \n    print(\"=\"*66 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:33:36.018550Z","iopub.execute_input":"2025-08-08T21:33:36.019215Z","iopub.status.idle":"2025-08-08T21:33:36.030533Z","shell.execute_reply.started":"2025-08-08T21:33:36.019186Z","shell.execute_reply":"2025-08-08T21:33:36.029624Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def is_truly_positive(label_path):\n    \"\"\"\n    Ð¡hecks if the labels file contains real data and not just spaces.\n    \"\"\"\n    if not label_path.exists():\n        return False\n    with open(label_path, 'r') as f:\n        for line in f:\n            if line.strip():\n                return True\n    return False\n\ndef prepare_aggregated_dataset(source_paths, dest_path, neg_pos_ratio):\n\n    \"\"\"\n    Aggregates, and balances a dataset from multiple sources for training YOLO models.\n\n    This function solves three key problems when working with large, partitioned datasets:\n    1.  **Aggregation:** Combines files from multiple input directories (`source_paths`) into a\n        single, unified structure.\n    2.  **Training Set Balancing:** Creates a training dataset with a controlled ratio of \"negative\" (no objects)\n        to \"positive\" (with objects) examples to combat false positives and accelerate training.\n    3.  **Preserving Real Validation Distribution:** The validation set remains untouched to ensure a fair\n        and realistic evaluation of the model's performance.\n\n    The process is executed in two passes for maximum reliability:\n    - **Pass 1 (Aggregation):** Scans all specified `source_paths`, analyzes label files,\n      and compiles complete lists of \"positive\" and \"negative\" images for the `train` and `val` splits.\n    - **Pass 2 (Sampling and Linking):** Based on the aggregated lists, it creates the final dataset\n      structure in `dest_path` using symbolic links (symlinks) to save disk space.\n\n    Args:\n        source_paths (list[Path]):\n            A list of paths (`pathlib.Path` objects) to the root directories of the input datasets.\n            Each dataset is expected to contain `train` and/or `val` subdirectories, which in turn\n            contain a mix of image (.jpg, .jpeg, .png) and label (.txt) files.\n\n        dest_path (Path):\n            The path to the target directory where the final, ready-to-train dataset structure\n            will be created. If the directory exists, it will be completely removed and recreated.\n\n        neg_pos_ratio (float):\n            The desired ratio of \"negative\" (background) to \"positive\" (with objects) examples\n            in the **training** set.\n            - `1.0` means a 1:1 ratio (one negative for each positive).\n            - `0.25` means a 1:4 ratio (one negative for every four positives).\n            - `0.0` will completely exclude background images from training.\n\n    Returns:\n        Path:\n            The absolute path to the generated `dataset.yaml` file, which can be passed\n            directly to the `train()` method of a YOLO model.\n\n    Raises:\n        ValueError: If `random.sample` cannot select the requested number of negative\n                    examples (though the code has a safeguard against this).\n    \"\"\"\n  \n    print(f\"\\nINFO: Setting up final dataset directory at {dest_path}\")\n    if dest_path.exists(): shutil.rmtree(dest_path)\n    images_train_dir, labels_train_dir = dest_path / \"images/train\", dest_path / \"labels/train\"\n    images_val_dir, labels_val_dir = dest_path / \"images/val\", dest_path / \"labels/val\"\n    for d in [images_train_dir, labels_train_dir, images_val_dir, labels_val_dir]:\n        d.mkdir(parents=True)\n        \n    VALID_IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\"}\n\n    print(\"--- PASS 1: Aggregating all data files ---\")\n    all_train_pos, all_train_neg = [], []\n    all_val_pos, all_val_neg = [], []\n\n    for source_path in source_paths:\n        print(f\"--> Analyzing {source_path.name}...\")\n        for split in [\"train\", \"val\"]:\n            source_split_dir = source_path / split\n            if not source_split_dir.exists(): continue\n            \n            image_files = [f for f in source_split_dir.glob(\"*\") if f.suffix.lower() in VALID_IMAGE_EXTENSIONS]\n            for img_file in tqdm(image_files, desc=f\"  - Reading {split} manifest\"):\n                label_file = img_file.with_suffix('.txt')\n                \n                is_positive = is_truly_positive(label_file)\n                if split == 'train':\n                    (all_train_pos if is_positive else all_train_neg).append(img_file)\n                else:\n                    (all_val_pos if is_positive else all_val_neg).append(img_file)\n\n    print(\"\\n--- Aggregation Complete ---\")\n    print(f\"Total Train Positives: {len(all_train_pos)}\")\n    print(f\"Total Train Negatives: {len(all_train_neg)}\")\n\n    print(\"\\n--- PASS 2: Sampling and Linking ---\")\n    num_neg_to_add = int(len(all_train_pos) * neg_pos_ratio)\n    num_neg_to_add = min(num_neg_to_add, len(all_train_neg))\n    print(f\"Linking all {len(all_train_pos)} positives and {num_neg_to_add} sampled negatives for TRAIN...\")\n    final_train_paths = all_train_pos + random.sample(all_train_neg, num_neg_to_add)\n    for img_path in tqdm(final_train_paths, desc=\"  - Linking train set\"):\n        lbl_path = img_path.with_suffix('.txt')\n        os.symlink(img_path, images_train_dir / img_path.name)\n        if lbl_path.exists(): os.symlink(lbl_path, labels_train_dir / lbl_path.name)\n    final_val_paths = all_val_pos + all_val_neg\n    print(f\"Linking all {len(final_val_paths)} images for VAL...\")\n    for img_path in tqdm(final_val_paths, desc=\"  - Linking val set\"):\n        lbl_path = img_path.with_suffix('.txt')\n        os.symlink(img_path, images_val_dir / img_path.name)\n        if lbl_path.exists(): os.symlink(lbl_path, labels_val_dir / lbl_path.name)\n        \n    yaml_path = dest_path / \"dataset.yaml\"\n    with open(yaml_path, 'w') as f:\n        yaml.dump({'path': str(dest_path.resolve()), 'train': 'images/train', 'val': 'images/val', 'nc': 1, 'names': ['person']}, f)\n    return yaml_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:33:39.020360Z","iopub.execute_input":"2025-08-08T21:33:39.020602Z","iopub.status.idle":"2025-08-08T21:33:39.032155Z","shell.execute_reply.started":"2025-08-08T21:33:39.020586Z","shell.execute_reply":"2025-08-08T21:33:39.031452Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device_count = torch.cuda.device_count()\n    print(f\"INFO: ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {device_count} Ð²Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚ NVIDIA.\")\n    \n    if device_count > 1:\n        device_ids = ','.join(map(str, range(device_count)))\n        print(f\"INFO: Ð‘ÑƒÐ´ÑƒÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð²ÑÐµ Ð²Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚Ñ‹: {device_ids}\")\n    elif device_count == 1:\n        device_ids = '0'\n        print(\"INFO: Ð‘ÑƒÐ´ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð¾Ð´Ð½Ð° Ð²Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚Ð°: 0\")\n    else:\n        device_ids = None\n        print(\"WARNING: Ð’Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚Ñ‹ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹, Ð½Ð¾ Ð¸Ñ… ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€Ð°Ð²Ð½Ð¾ 0. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ CPU.\")\nelse:\n    device_ids = None\n    print(\"WARNING: Ð’Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚Ñ‹ NVIDIA Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:33:45.212674Z","iopub.execute_input":"2025-08-08T21:33:45.212938Z","iopub.status.idle":"2025-08-08T21:33:45.329774Z","shell.execute_reply.started":"2025-08-08T21:33:45.212916Z","shell.execute_reply":"2025-08-08T21:33:45.329006Z"}},"outputs":[{"name":"stdout","text":"INFO: ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ 2 Ð²Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚ NVIDIA.\nINFO: Ð‘ÑƒÐ´ÑƒÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð²ÑÐµ Ð²Ð¸Ð´ÐµÐ¾ÐºÐ°Ñ€Ñ‚Ñ‹: 0,1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"EPOCHS = 100\nRATIO = 1.25\n\nINPUT_DATASET_PATHS = [\n    Path(\"/kaggle/input/uav-people-sliced-1536px-slices-20-of-data\"),\n    Path(\"/kaggle/input/uav-people-sliced-1536px-slices-80-of-data\"),\n]\nFINAL_DATASET_DIR = Path(\"/kaggle/working/final_prepared_dataset\")\n\nfinal_yaml_path = prepare_aggregated_dataset(INPUT_DATASET_PATHS, FINAL_DATASET_DIR, RATIO)\n\nmodel = YOLO(\"yolo12s.pt\") \nmodel.add_callback(\"on_epoch_end\", custom_metric_callback)\n\nmodel.train(\n    data=str(final_yaml_path),\n    epochs=EPOCHS,\n    imgsz=640,\n    patience=1,\n    batch=20,\n    device=device_ids,\n    project=\"uav_sliced_on_the_fly\",\n    name=\"run_continuous\",\n    val=True,\n    save=True,\n    exist_ok=True,\n    pretrained=True, # Use pretrained weights if available\n    optimizer=\"AdamW\",\n    single_cls=True, # We are only detecting people\n    rect=True, # Use rectangular training to speed up training\n    cos_lr=True, # Use cosine learning rate scheduler\n    close_mosaic=10, # Disable mosaic augmentation on last 10 epochs\n    resume=False, # Start training from scratch\n    amp=False, # Use automatic mixed precision for faster training\n    fraction=1.0, # Use 100% of the dataset\n    lr0=0.01,\n    weight_decay=0.0005,\n\n    hsv_h=0.015, # Hue augmentation percentage range\n    hsv_s=0.70, # Saturation augmentation percentage range\n    hsv_v=0.40, # Brightness augmentation percentage range\n    degrees=0.0, # No rotation augmentation\n    translate=0.05, # Translation augmentation percentage range (keeping it low to avoid losing small objects)\n    scale=0.60, # Scale augmentation percentage range\n    shear=0.0, # No shear augmentation\n    perspective=0.0, # No perspective augmentation\n    flipud=0.0, # No vertical flip augmentation\n    fliplr=0.50, # Horizontal flip augmentation probability\n    bgr=0.0, # No RGB->BGR channel swapping\n    mosaic=0.8, # Mosaic augmentation probability (combines 4 images into one)\n    mixup=0.0, # Mixup augmentation probability (combines 2 images into one)\n    cutmix=0.0, # CutMix augmentation probability (cuts and pastes patches from one image to another)\n    copy_paste=0.2, \n    erasing=0.2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T21:42:49.188199Z","iopub.execute_input":"2025-08-08T21:42:49.188483Z"}},"outputs":[{"name":"stdout","text":"\nINFO: Setting up final, BALANCED dataset directory at /kaggle/working/final_prepared_dataset\n--- PASS 1: Aggregating all data files ---\n--> Analyzing uav-people-sliced-1536px-slices-20-of-data...\n","output_type":"stream"},{"name":"stderr","text":"  - Reading train manifest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109846/109846 [17:00<00:00, 107.64it/s]\n  - Reading val manifest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5396/5396 [00:48<00:00, 112.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"--> Analyzing uav-people-sliced-1536px-slices-80-of-data...\n","output_type":"stream"},{"name":"stderr","text":"  - Reading train manifest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439388/439388 [1:17:25<00:00, 94.57it/s]  \n  - Reading val manifest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21584/21584 [03:43<00:00, 96.69it/s] \n","output_type":"stream"},{"name":"stdout","text":"\n--- Aggregation Complete ---\nTotal Train Positives: 76547\nTotal Train Negatives: 472687\n\n--- PASS 2: Sampling and Linking ---\nLinking all 76547 positives and 95683 sampled negatives for TRAIN...\n","output_type":"stream"},{"name":"stderr","text":"  - Linking train set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172230/172230 [06:57<00:00, 412.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Linking all 26980 images for VAL...\n","output_type":"stream"},{"name":"stderr","text":"  - Linking val set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26980/26980 [00:34<00:00, 771.39it/s] \nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12s.pt to 'yolo12s.pt': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.1M/18.1M [00:00<00:00, 86.6MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Ultralytics 8.3.176 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                        CUDA:1 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=20, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.2, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/final_prepared_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.2, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.937, mosaic=0.8, multi_scale=False, name=run_continuous, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=1, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=uav_sliced_on_the_fly, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=uav_sliced_on_the_fly/run_continuous, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.6, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 18.8MB/s]","output_type":"stream"},{"name":"stdout","text":"Overriding class names with single class.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n 21        [14, 17, 20]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \nYOLOv12s summary: 272 layers, 9,253,523 parameters, 9,253,507 gradients, 21.5 GFLOPs\n\nTransferred 685/691 items from pretrained weights\nWARNING âš ï¸ 'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'\n\u001b[34m\u001b[1mDDP:\u001b[0m debug command /usr/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 57651 /root/.config/Ultralytics/DDP/_temp_l2gbsfid135544172437072.py\nUltralytics 8.3.176 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                        CUDA:1 (Tesla T4, 15095MiB)\nOverriding class names with single class.\nWARNING âš ï¸ ClearML installed but not initialized correctly, not logging this run. It seems ClearML is not configured on this machine!\nTo get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\nSetup instructions can be found here: https://clear.ml/docs\nOverriding model.yaml nc=80 with nc=1\nTransferred 685/691 items from pretrained weights\nFreezing layer 'model.21.dfl.conv.weight'\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.5Â±0.1 ms, read: 27.0Â±15.8 MB/s, size: 356.5 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/final_prepared_dataset/labels/train... 172230 images, 95683 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172230/172230 [20:51<00:00, 137.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/final_prepared_dataset/labels/train.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.8Â±3.2 ms, read: 38.3Â±16.2 MB/s, size: 482.9 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/final_prepared_dataset/labels/val... 26980 images, 24388 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26980/26980 [02:41<00:00, 166.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/final_prepared_dataset/labels/val.cache\nPlotting labels to uav_sliced_on_the_fly/run_continuous/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00046875), 119 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1muav_sliced_on_the_fly/run_continuous\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      6.14G      1.902       1.75      1.339          9        640:  31%|â–ˆâ–ˆâ–ˆ       | 2630/8612 [32:02<1:05:30,  1.52it/s]","output_type":"stream"}],"execution_count":null}]}